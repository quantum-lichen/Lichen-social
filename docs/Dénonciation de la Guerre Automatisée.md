# **L’Automne de l’Humanité : Rapport Exhaustif sur l’Automatisation de la Guerre et l’Érosion du Principe de Distinction**

Soumis au : Secrétariat Général de l’Organisation des Nations Unies (ONU)  
À l’attention de : Groupe d’experts gouvernementaux (GGE) sur les systèmes d’armes létaux autonomes (SALA)  
Date : 1 Janvier 2026  
Objet : Dénonciation de l’automatisation des conflits armés, de la transformation de facto des populations civiles en cibles statistiques, et appel à une action immédiate fondé sur les principes de la conscience publique et l'esthétique de la vérité.

## ---

**1\. Introduction : Le Silence de la Neige et le Bruit de la Machine**

La guerre, dans sa conception classique et tragique, a toujours été une confrontation de volontés humaines. Elle est brutale, elle est le théâtre de l'horreur, mais elle demeure, jusqu'à présent, une expérience régie par la peur, le courage, l'erreur, la pitié et la responsabilité morale. L'histoire militaire est parsemée d'instants où le soldat, face à l'ennemi, perçoit une humanité partagée — un regard, une hésitation, la présence d'un enfant — et décide de suspendre son tir. Cet espace infime entre la perception et l'action est le lieu où réside la moralité de la guerre. C'est dans cet intervalle que le Droit International Humanitaire (DIH) trouve son ancrage réel : non pas dans les textes, mais dans la conscience de celui qui tient l'arme.

Aujourd'hui, nous assistons à une transformation silencieuse, radicale et terrifiante : le transfert de la décision de tuer de l'homme à la machine. Ce rapport, exhaustif dans son détail et rigoureux dans son analyse, vise à démontrer devant cette assemblée que l'automatisation de la guerre ne constitue pas une simple évolution technique vers plus d'efficacité, mais une rupture morale et juridique absolue. En automatisant la violence, les gouvernements du monde ne créent pas des guerres plus "propres" ou plus "précises" ; ils créent des abattoirs algorithmiques où le civil, par sa nature même de donnée non-combattante, devient la cible par défaut de systèmes incapables de saisir la nuance vibrante de l'existence.

Pour articuler cette dénonciation, ce document s'ancre profondément dans l'esthétique et la philosophie du *haïku*. Pourquoi le haïku? Parce que cette forme poétique japonaise est une méthode de vérité. Le haïku repose sur le *shasei* (le croquis sur le vif, la description objective de la nature), le *kireji* (la coupe qui juxtapose deux images pour révéler une vérité cachée) et le *mono no aware* (la conscience aigüe de l'impermanence des choses).1 Face à l'abstraction des algorithmes qui réduisent la vie humaine à des vecteurs de données et à des probabilités statistiques, nous opposons la clarté du haïku : la réalité concrète, fragile, irremplaçable et non numérisable d'une vie humaine.

L'automatisation supprime le "moment haïku" du combat — cet instant de flottement et de lucidité. La machine ne connaît pas l'hésitation. Elle ne connaît que l'exécution d'un code. En supprimant l'hésitation, nous supprimons l'humanité de la guerre. Et une guerre sans humanité n'est plus une guerre : c'est une éradication industrielle.

Ce rapport se déploiera en quatre mouvements, ou "saisons" de l'analyse, allant de la froideur technique des algorithmes à la chaleur du sang versé, pour aboutir à l'impératif moral de l'interdiction totale des systèmes autonomes ciblant les êtres humains. Nous y examinerons les preuves accablantes des défaillances technologiques, les précédents historiques des guerres de drones, et les récentes révélations sur l'usage de l'intelligence artificielle (IA) dans les conflits urbains, démontrant que la promesse de précision est un mensonge qui masque une réalité de massacres de masse.

## ---

**2\. L'Hiver Algorithmique : La Mécanique de la Déshumanisation**

L'argument central des puissances militaires développant des Systèmes d'Armes Létaux Autonomes (SALA) — notamment les États-Unis, la Russie, la Chine et Israël — repose sur une promesse d'hygiène : la guerre chirurgicale. On nous dit que l'Intelligence Artificielle, libérée de la peur, de la colère, de la fatigue et du désir de vengeance, sera plus précise et plus respectueuse du droit que l'être humain.3 Les données empiriques et la réalité technique de l'apprentissage automatique (Machine Learning) prouvent le contraire. L'hiver algorithmique est celui d'une vision figée, incapable de distinguer la vie de la simulation, transformant le champ de bataille en un immense tableau de données où l'exception est écrasée par la règle.

### **2.1. L'Illusion de la Précision et le Biais Intégré**

La technologie de reconnaissance de cibles, qu'elle soit basée sur la vision par ordinateur (Computer Vision), l'imagerie thermique ou l'analyse des métadonnées, n'est pas neutre. Elle est le produit de ses données d'entraînement, et ces données portent en elles les stigmates des préjugés humains, amplifiés par la rigidité mathématique.

#### **2.1.1. La Cécité Raciale et de Genre des Réseaux de Neurones**

Les systèmes de vision par ordinateur, qui constituent l'œil de la machine autonome, souffrent de biais structurels profonds documentés par de nombreuses études académiques. La reconnaissance faciale et la détection d'objets, piliers de l'identification militaire, échouent disproportionnellement sur certaines populations.

* Une étude séminale du MIT Media Lab, intitulée "Gender Shades", a révélé que les algorithmes commerciaux de reconnaissance faciale présentaient des taux d'erreur massivement plus élevés pour les personnes à la peau foncée.5  
* Le taux d'erreur pour les hommes à la peau claire était de **0,8 %**.  
* Pour les femmes à la peau foncée, ce taux grimpait à **34,7 %**.5

Dans un contexte civil, ce biais se traduit par une porte qui ne s'ouvre pas ou une identification erronée sur un réseau social. Dans un contexte militaire, où la décision de tir se joue en millisecondes et où la machine a l'autonomie de l'engagement, ce "biais" se traduit par une frappe de missile. Transposons ces statistiques sur un théâtre d'opérations en Afrique sub-saharienne ou au Moyen-Orient. Un système autonome programmé pour identifier des combattants dans une population majoritairement non-blanche part avec un handicap cognitif structurel. Il "voit" moins bien ceux qu'il est censé épargner ou cibler. La technologie encode les préjugés de ses créateurs et les limites de ses bases de données d'entraînement, souvent constituées d'images occidentales (comme le jeu de données ImageNet, standard de l'industrie).6

De plus, les biais ne sont pas seulement morphologiques, mais contextuels. Les algorithmes d'apprentissage profond (Deep Learning) fonctionnent par corrélation. Si les données d'entraînement associent majoritairement des hommes barbus en tenue traditionnelle à des "insurgés", l'algorithme apprendra cette corrélation comme une règle de causalité.7 Il ne voit pas un religieux, un père ou un commerçant ; il voit un ensemble de pixels dont la probabilité statistique correspond à la classe "cible".

Vieux chêne sous la neige \-  
L'œil de verre ne voit  
Que l'ombre d'un fusil.

Le *haïku* nous enseigne l'importance du détail unique : une goutte de rosée, une feuille qui tombe, la spécificité de l'instant. L'IA, elle, généralise. Elle fonctionne par reconnaissance de *patterns* (motifs). Or, la guerre irrégulière moderne est le domaine de l'exception, non de la règle. Un combattant peut lâcher son arme pour se rendre. Un civil peut ramasser une arme pour la déplacer ou se défendre contre un crime de droit commun. Pour un réseau de neurones convolutif (CNN) tel que YOLO-v8, utilisé dans la détection de cibles en temps réel 8, ces nuances sont invisibles. Le système voit "forme humaine \+ objet allongé \= menace". Il n'y a pas de contexte social, seulement une corrélation de géométrie et de pixels.

#### **2.1.2. La Vulnérabilité aux Attaques Adversariales et au "Bruit" du Monde**

Au-delà des biais inhérents, les systèmes de vision par ordinateur sont extrêmement fragiles face aux modifications de leur environnement. La recherche en télédétection militaire montre que les conditions atmosphériques, la lumière, ou des modifications délibérées (camouflages adversariaux) peuvent tromper les algorithmes avec une facilité déconcertante.

* Des "patchs adversariaux" (images imprimées portées sur des vêtements) peuvent rendre une personne invisible aux détecteurs d'objets ou, pire, la faire classer comme une autre entité (par exemple, identifier un humain comme une chaise ou un avion).10  
* Dans les conflits asymétriques, où les combattants n'ont pas d'uniformes distincts, la distinction repose sur des indices subtils. Les systèmes SAR (Radar à Synthèse d'Ouverture) ou infrarouges détectent la chaleur ou la densité des matériaux 11, mais sont incapables de comprendre l'intentionnalité d'un geste.

Un algorithme ne comprend pas la différence entre un tuyau d'irrigation porté sur l'épaule et un lance-roquettes (RPG). Pour la machine, la signature visuelle est similaire. Dans l'urgence du calcul, la prudence humaine (le doute) est remplacée par un seuil de confiance statistique. Si le seuil est dépassé (par exemple 85%), le tir est autorisé. L'erreur n'est pas une anomalie du système ; elle est une caractéristique statistique acceptée par sa conception même.

### **2.2. L'Impossibilité du Jugement Contextuel et le Principe de Distinction**

Le principe de distinction, pierre angulaire du Droit International Humanitaire (DIH), exige des parties au conflit de différencier à tout moment combattants et civils, et de ne diriger les attaques que contre des objectifs militaires. Les défenseurs des SALA, notamment les délégations russes et américaines au CCW, arguent que les capteurs modernes peuvent effectuer cette distinction mieux que les humains stressés.3 C'est une erreur fondamentale de catégorie philosophique et juridique.

La distinction n'est pas une mesure optique ; c'est un jugement moral et contextuel complexe.

* **Juxtaposition (Kireji)** :  
  * *L'œil du drone : pixels thermiques, silhouette de métal, mouvement rapide.*  
  * *La réalité : un père courant avec son enfant fiévreux vers l'hôpital.*

La machine superpose ces deux réalités et tire, car son modèle prédictif associe "course rapide \+ zone de combat \+ objet dans les bras" à "combattant repositionnant une arme". Le "bruit" de la vie quotidienne dans une zone de guerre — les mariages, les funérailles, le commerce, les jeux des enfants — est interprété par l'algorithme comme une "signature" suspecte.

Les "frappes de signature" (signature strikes), pratiquées depuis des années par les drones américains au Pakistan et au Yémen, sont les ancêtres directs et imparfaits des SALA.13 Elles ne visent pas des individus identifiés nommément, mais des comportements correspondant à une "signature" de terroriste.

* En 2013, un drone américain a frappé un cortège de mariage à Radda, au Yémen, tuant 12 civils. Les véhicules roulaient en file (comportement suspect pour l'algorithme de surveillance) et transportaient des hommes armés (coutume traditionnelle de mariage au Yémen). L'absence de compréhension culturelle et contextuelle a transformé une fête en massacre.15

L'automatisation pousse cette logique à son paroxysme : nous ne tuons plus des ennemis, nous "traitons" des profils de données. La guerre ne se fait plus contre des sujets politiques, mais contre des anomalies statistiques.

### **2.3. La Table de la Discordance : Jugement Humain vs Calcul Algorithmique**

Pour illustrer le fossé infranchissable entre la cognition humaine et le traitement machine, nous présentons la comparaison suivante, basée sur les recherches en psychologie cognitive et en intelligence artificielle militaire.7

| Dimension du Ciblage | Jugement Humain (Idéal du DIH) | Calcul Algorithmique (Réalité Technique Actuelle) | Conséquence pour les Populations Civiles |
| :---- | :---- | :---- | :---- |
| **Identification** | Visuelle, contextuelle, empathique. Capacité intrinsèque au doute et à l'hésitation. | Reconnaissance de formes (*Pattern Matching*), corrélation statistique. Absence de doute, seulement un seuil de confiance binaire. | Les objets civils (outils agricoles, béquilles, jouets) sont confondus avec des armes. Biais racial et de genre massif. |
| **Intention** | Analyse du comportement, des signes de reddition, de peur ou de détresse. Interprétation culturelle des gestes. | Analyse cinématique (vitesse, direction). L'immobilité ou la fuite sont des vecteurs géométriques, pas des états émotionnels. | Un civil fuyant une zone de combat ou se cachant est interprété comme une cible hostile (comportement d'évasion). |
| **Proportionnalité** | Évaluation morale et juridique du coût humain versus l'avantage militaire concret et direct attendu. | Calcul utilitaire froid. Paramétrage d'un nombre de "dommages collatéraux" tolérés (ex: \< 15 civils). | Acceptation automatique de massacres familiaux pour des cibles mineures, si le quota le permet (Cas Lavender). |
| **Responsabilité** | Individuelle (le soldat, le commandant). Risque pénal, traumatisme moral, syndrome de stress post-traumatique. | Diffuse et diluée. "Erreur système", "bug", "faux positif". Pas de traumatisme, pas de Cour Martiale pour un algorithme. | Impunité de fait. La guerre devient sans risque physique ou moral pour l'attaquant, et totale pour le défenseur. |

## ---

**3\. Le Printemps Brisé : Le Cas "Lavender" et la Preuve par le Sang**

Si les débats au sein du CCW à Genève restent souvent théoriques, l'histoire récente nous a fourni une preuve empirique terrifiante de ce qui attend l'humanité si l'automatisation n'est pas endiguée. L'enquête approfondie publiée par *\+972 Magazine* et *Local Call* sur l'utilisation de systèmes d'IA par l'armée israélienne à Gaza en 2023-2024 constitue le "moment Hiroshima" de la guerre algorithmique.18 Ce n'est plus de la science-fiction ; c'est l'histoire immédiate et sanglante.

### **3.1. L'Industrialisation de la Cible : Le Système Lavender**

Le système "Lavender" a été conçu pour traiter des quantités massives de données de surveillance et générer des listes de cibles humaines à une vitesse qu'aucun analyste humain ne pourrait égaler.

* **L'Objectif :** Résoudre le "goulot d'étranglement humain" dans la localisation de nouvelles cibles. La machine sert à remplir la "banque de cibles" plus vite qu'elles ne peuvent être détruites.18  
* **Le Chiffre :** Dans les premières semaines du conflit, le système a identifié et marqué jusqu'à **37 000** Palestiniens comme militants présumés du Hamas ou du Jihad Islamique, les désignant pour des frappes aériennes potentielles.18  
* **Le Taux d'Erreur Connu :** Les opérateurs militaires savaient, par des tests internes, que le système avait un taux d'erreur d'environ **10 %**.21 Cela signifie que sur 37 000 cibles, environ 3 700 individus étaient potentiellement des civils sans lien avec les combats — identifiés par erreur à cause d'un homonyme, d'un changement de téléphone, ou d'une ressemblance de profil comportemental.  
* **La Politique :** L'armée a sciemment accepté ce taux d'erreur. Les protocoles ont été assouplis pour permettre l'automatisation de la mort de milliers d'innocents au nom de l'efficacité opérationnelle.18

Ici réside la rupture éthique fondamentale. Un commandant humain qui accepterait sciemment un taux d'erreur de 10 % sur une liste de 37 000 personnes commettrait un crime de guerre manifeste, passible de la Cour Pénale Internationale. La machine, elle, dilue cette responsabilité dans l'opacité de la "boîte noire". Elle transforme le crime en statistique.

### **3.2. "Where's Daddy" : L'Horreur Domestique et l'Anti-Haïku**

Si Lavender identifie *qui* tuer, le système auxiliaire, cyniquement nommé "Where's Daddy?" (Où est papa?), détermine *quand* et *où* tuer. Ce système illustre la perversion morale absolue de l'automatisation.18 "Where's Daddy?" traquait les cibles identifiées par Lavender pour signaler spécifiquement le moment où elles rentraient dans leur foyer familial.

* **La Logique :** Il est plus facile, techniquement, de localiser une cible chez elle, immobile, que sur le champ de bataille mouvant. De plus, les cibles "junior" ne valaient pas le coût d'une frappe complexe en extérieur.  
* **La Conséquence :** L'armée a systématisé le bombardement de domiciles privés la nuit. Le système attendait délibérément que la cible soit avec sa famille pour frapper.  
* **Le Résultat :** Pour tuer un seul militant présumé (parfois de très bas rang), l'armée a accepté de tuer des familles entières. Des témoignages d'officiers rapportent des ratios de "dommages collatéraux" autorisés allant jusqu'à **15 ou 20 civils** pour un seul opérateur junior.18 Pour un commandant, ce chiffre pouvait monter à plus de 100 civils.18

C'est l'anti-haïku absolu. Le haïku célèbre le foyer, l'instant de paix domestique, le thé partagé, le refuge contre la tempête.

* *Soirée d'hiver \-*  
  * *Le bruit des clés dans la serrure,*  
  * *Devient le signal de la bombe.*

L'automatisation a transformé le refuge le plus sacré du civil — sa maison — en piège mortel. En automatisant la surveillance, on a automatisé la violation du principe de proportionnalité. L'algorithme a calculé que la valeur militaire d'un combattant junior valait la vie de sa femme, de ses enfants et de ses voisins. C'est une comptabilité macabre que seule une machine (ou un esprit devenu machine) peut effectuer sans s'effondrer.

### **3.3. L'Accélération et la Perte de Contrôle Humain : Le Mythe du "Human in the Loop"**

L'argument de la "boucle de contrôle humaine" (*human-in-the-loop*), souvent avancé par les États pour rassurer l'opinion publique 4, s'effondre face à la vitesse et au volume de la guerre algorithmique.

* Dans le cas de Lavender, les opérateurs humains ne consacraient en moyenne que **20 secondes** à la vérification de chaque cible avant d'autoriser la frappe.22  
* Ce temps servait principalement à vérifier si la cible était un homme (sexe masculin). Il n'y avait pas d'analyse approfondie des renseignements, pas de contre-enquête.  
* Le rôle de l'humain s'est réduit à celui d'un "tampon en caoutchouc" (*rubber stamp*).22

Vingt secondes pour juger de la fin d'une vie et de celle d'une famille. C'est moins de temps qu'il n'en faut pour lire et méditer un haïku. C'est une parodie de contrôle. L'humain ne décide plus ; il valide une décision déjà prise par la machine, écrasé par le volume de données et le phénomène psychologique bien documenté du "biais d'automatisation" (*automation bias*), qui nous pousse à faire aveuglément confiance aux résultats d'un ordinateur.21 L'automatisation crée une bureaucratie de la mort où personne n'est véritablement responsable : ni le programmeur (qui n'a pas appuyé sur le bouton), ni l'opérateur (qui a suivi la recommandation fiable à 90%), ni le commandant (qui s'est fié au système validé par l'état-major).

## ---

**4\. L'Été de la Cible : Le Civil comme Seule Réalité Tangible**

Dans la logique algorithmique, le "combattant" n'est plus un soldat en uniforme portant un drapeau. C'est un assemblage de métadonnées, une construction probabiliste. Si vous appelez telle personne, si votre téléphone borne sur telle tour relais, si vous changez de carte SIM tous les deux jours, vous devenez une cible. Or, dans les zones de conflit modernes (Gaza, Yémen, Ukraine, Sahel), les civils et les combattants partagent le même espace, les mêmes réseaux de télécommunication, parfois les mêmes ressources de survie.

### **4.1. Le "Pattern of Life" et la Fin de l'Innocence**

L'analyse des "motifs de vie" (*pattern of life*) est la méthode statistique utilisée pour prédire la menace.24 Elle repose sur la régularité et l'anomalie. Mais la vie humaine en temps de guerre est fondamentalement irrégulière.

* Un cortège de mariage au Yémen, roulant lentement en file dans le désert, ressemble structurellement à un convoi militaire pour un drone analysant la cinématique des véhicules.15  
* Des villageois se rassemblant sous un arbre pour discuter d'un litige foncier ressemblent à une réunion de commandement tactique.

L'algorithme ne connaît pas la coutume. Il ne connaît pas le rituel. Il ne connaît pas la culture. Il voit une anomalie vectorielle.

* *Pétales de cerisier tombent,*  
  * *Le cortège de la mariée*  
  * *Confondus avec la cendre.*

En automatisant l'interprétation de ces motifs, nous condamnons l'innocence à être perpétuellement suspecte. Il n'y a plus de présomption de statut civil ; il y a une présomption de culpabilité algorithmique jusqu'à preuve du contraire (souvent post-mortem). Le système fonctionne selon une logique d'exclusion : tout ce qui ressemble à la cible est traité comme tel.

### **4.2. La Déshumanisation Numérique (Digital Dehumanization)**

La campagne "Stop Killer Robots" et d'autres ONG ont forgé le concept de "déshumanisation numérique".26 Ce terme décrit le processus par lequel les humains sont réduits à des points de données, dépouillés de leur complexité biographique et biologique.

* Dans le système Lavender, les individus reçoivent une note de 1 à 100 indiquant leur probabilité d'être un combattant.18  
* À partir d'un certain seuil (fixé arbitrairement par le commandement), la machine recommande l'élimination.  
* L'être humain n'est plus une personne juridique titulaire de droits inaliénables ; il est un score, une ligne dans une base de données SQL.

Cette réduction ontologique est le prélude nécessaire à tout massacre de masse. L'histoire des génocides nous montre que pour exterminer un groupe, il faut d'abord le déshumaniser (les traiter de "cafards", de "rats"). L'IA facilite cette distanciation psychologique à une échelle industrielle et aseptisée. Elle permet de tuer à distance, non seulement géographique (comme le permettaient déjà l'artillerie ou l'aviation), mais cognitive et morale. L'opérateur ne tue pas un homme ; il "valide une cible". Le vocabulaire même change pour masquer la réalité sanglante : on parle de "traitement", "d'engagement", de "neutralisation", jamais de mort.

### **4.3. L'Érosion du Statut de Civil**

Le résultat de cette automatisation est que le civil devient, *de facto*, la seule cible réelle. Pourquoi?

1. **Le Nombre :** Les civils sont toujours plus nombreux que les combattants. Statistiquement, un algorithme générant des faux positifs (erreurs) frappera donc majoritairement des civils.  
2. **La Visibilité :** Les combattants se cachent, se camouflent, utilisent des contre-mesures. Les civils doivent vivre, se déplacer pour trouver de l'eau, de la nourriture, s'occuper de leurs familles. Ils sont "visibles" pour les capteurs.28  
3. **L'Association :** Dans les guerres urbaines, la proximité physique entre civils et combattants (vivant dans les mêmes immeubles) signifie que tout ciblage algorithmique basé sur la géolocalisation inclura nécessairement les civils.18

Ainsi, l'automatisation de la guerre aboutit à une inversion perverse : la technologie promise pour "sauver des vies innocentes" grâce à la précision devient la machine la plus efficace pour les détruire en masse.

## ---

**5\. L'Automne de la Diplomatie : L'Impasse du CCW et l'Urgence de 2026**

Face à cette catastrophe imminente, la réponse de la communauté internationale a été tragiquement lente et entravée par la realpolitik.

### **5.1. L'Échec du Forum CCW**

Depuis 2014, le Groupe d'experts gouvernementaux (GGE) de la Convention sur certaines armes classiques (CCW) débat des SALA à Genève. Plus d'une décennie de discussions n'a abouti à aucun instrument contraignant.29

* **Les Bloqueurs :** Une minorité d'États militaristes, principalement la **Russie**, les **États-Unis**, Israël, l'Inde et la Corée du Sud, bloquent systématiquement tout progrès vers un traité d'interdiction.30  
* **L'Argument Russe :** La Russie soutient qu'il n'y a pas de "preuves convaincantes" de la nécessité de nouvelles lois, que le DIH existant suffit, et que toute régulation entraverait le progrès technologique civil.31 La Russie utilise le consensus (droit de veto de fait au CCW) pour paralyser les débats.  
* **L'Argument Américain :** Les États-Unis, bien que plus ouverts au dialogue sur les "risques", promeuvent des "bonnes pratiques" non contraignantes et des codes de conduite volontaires, refusant le terme "interdiction".4 Ils insistent sur le fait que l'autonomie peut améliorer le respect du DIH en réduisant l'erreur humaine — un argument que nous avons déconstruit plus haut.

Ce blocage est cynique. Pendant que les diplomates débattent de la définition exacte du "contrôle humain significatif" dans des salles climatisées à Genève, les systèmes comme Lavender sont déployés sur le terrain et tuent des milliers de personnes. Le forum du CCW est devenu le cimetière de la régulation, un écran de fumée permettant aux puissances de développer leurs armes sans entraves juridiques.

### **5.2. L'Appel pour 2026 et la Coalition des Volontaires**

Face à cette impasse, une coalition grandissante d'États (Autriche, Brésil, Chili, Mexique, Nouvelle-Zélande, etc.), d'ONG (la Campagne *Stop Killer Robots*, Human Rights Watch, Amnesty International) et d'institutions internationales (CICR) exige un traité contraignant d'ici **2026**.34

* Le Secrétaire Général de l'ONU, António Guterres, a lancé un ultimatum : il faut un instrument juridique d'ici 2026 pour interdire les armes qui fonctionnent sans contrôle humain.34  
* La présidente du CICR, Mirjana Spoljaric, appelle à tracer une "ligne rouge morale" et juridique.35

Il n'y a plus de temps pour les demi-mesures. La technologie avance selon la loi de Moore, exponentiellement plus vite que la diplomatie qui avance au rythme des consensus onusiens. Si nous n'agissons pas maintenant, le "fait accompli" technologique rendra tout retour en arrière impossible. Une fois les armées équipées et entraînées avec ces systèmes, elles ne les abandonneront plus.

## ---

**6\. L'Hiver de la Conscience : L'Impératif Moral et la Clause de Martens**

Au-delà des arguments techniques et juridiques, il existe un argument métajuridique fondamental : celui de la moralité publique.

### **6.1. La Clause de Martens : Un Bouclier contre la Barbarie Technique**

Le Droit International Humanitaire, conscient de ses propres lacunes face à l'avenir, a prévu un mécanisme de sécurité : la **Clause de Martens**. Introduite en 1899, elle stipule que dans les cas non couverts par les traités en vigueur, les populations civiles et les combattants restent sous la protection et l'empire "des principes de l'humanité et des exigences de la conscience publique".3

L'automatisation de la mise à mort viole frontalement ces deux piliers sacrés.

#### **6.1.1. Les Principes de l'Humanité**

Déléguer la mort à une machine est un acte de lâcheté morale qui nie la dignité humaine intrinsèque. Comme l'a souligné le regretté Christof Heyns, ancien Rapporteur spécial de l'ONU, "les machines n'ont pas de morale, pas de mortalité, et donc pas de capacité à comprendre la valeur de la vie".38 Tuer un être humain sans qu'un autre être humain n'en assume le fardeau émotionnel et moral est une atteinte à la dignité de la victime. C'est être tué par un objet, traité comme un nuisible à éliminer, et non comme un ennemi à respecter. Le droit à la vie inclut le droit de ne pas être tué par un algorithme.

#### **6.1.2. Les Exigences de la Conscience Publique**

La "conscience publique" mondiale s'est exprimée sans ambiguïté.

* Le Secrétaire Général de l'ONU a qualifié les SALA de "politiquement inacceptables et moralement répugnants".40  
* Le Pape François a consacré son message pour la paix 2024 aux risques de l'IA, avertissant contre une technologie "dépourvue de cœur" et appelant à ne pas laisser les algorithmes décider du destin des peuples.42  
* Des milliers de scientifiques de l'IA et de la robotique ont signé des lettres ouvertes demandant l'interdiction de ces armes.44

Ignorer ces voix convergentes — religieuses, laïques, scientifiques, politiques — pour poursuivre une course aux armements technologiques est une trahison de la conscience humaine universelle.

### **6.2. Le Principe du Haïku : L'Éthique de l'Attention et du *Satori***

Pourquoi invoquer le haïku dans un rapport juridique destiné à l'ONU? Parce que le haïku est l'art de l'attention extrême.  
Le poète de haïku s'arrête. Il suspend le temps. Il regarde. Il voit la grenouille sauter dans l'étang (furu ike ya). Il entend le bruit de l'eau. Il ressent la fraîcheur de l'instant.1 Ce moment d'attention pure, ce satori (éveil), est précisément ce qui manque à la machine.  
La moralité exige l'attention. Elle exige de voir le visage de l'autre, non comme une donnée biométrique à traiter, mais comme une énigme infinie, un "autre soi-même". L'automatisation est une inattention radicale. Elle est une vitesse qui empêche la vision. Elle est une accélération qui rend l'éthique impossible, car l'éthique demande le temps de la réflexion.

Lame de l'algorithme \-  
Coupe la branche et l'oiseau,  
Sans entendre le chant.

Si nous perdons la capacité de nous arrêter avant de frapper, nous perdons notre âme. L'automatisation de la guerre n'est pas une avancée militaire ; c'est une régression spirituelle. C'est la victoire de la logistique sur l'éthique, de la quantité sur la qualité, de la mort sur la vie.

## ---

**7\. Recommandations et Appel à l'Action : Le Dernier Haïku**

Le monde se trouve à la croisée des chemins. Une voie mène à une déresponsabilisation totale de la violence, où la guerre devient un processus autonome de gestion de stocks humains par des algorithmes opaques, une "nécropolitique" automatisée. L'autre voie, difficile mais nécessaire, réaffirme la primauté de l'esprit humain sur la matière et la technologie.

Nous recommandons solennellement à l'Assemblée Générale et au Conseil de Sécurité de l'ONU, ainsi qu'à tous les États parties au CCW, d'adopter les mesures suivantes :

1. **Négocier un Traité Juridiquement Contraignant (Échéance 2026\)** : Si le CCW reste bloqué par le consensus, les États volontaires doivent lancer un processus indépendant (sur le modèle du Processus d'Ottawa pour les mines antipersonnel ou d'Oslo pour les armes à sous-munitions) pour interdire les SALA.  
2. **Interdiction du Ciblage des Personnes** : Le traité doit inclure une interdiction absolue des systèmes d'armes autonomes conçus ou utilisés pour cibler des êtres humains. La gestion algorithmique de la létalité contre les personnes doit être proscrite.  
3. **Contrôle Humain Significatif (Meaningful Human Control)** : Le traité doit définir juridiquement le "contrôle" non comme une validation formelle de 20 secondes, mais comme une compréhension contextuelle, cognitive et une responsabilité active de chaque attaque individuelle. L'humain doit comprendre *pourquoi* la cible est choisie et pouvoir annuler l'action.  
4. **Interdiction des "Boîtes Noires"** : Interdire l'usage de l'IA générative ou de systèmes d'apprentissage profond non explicables dans la chaîne de commandement et de contrôle des armes nucléaires et conventionnelles. On ne peut pas confier la guerre à un système dont on ne comprend pas le raisonnement interne.  
5. **Responsabilité Pénale Individuelle** : Réaffirmer que les humains (commandants, opérateurs, programmeurs) restent pénalement responsables des crimes de guerre commis par les systèmes qu'ils déploient. L'excuse du "bug" ne doit jamais être recevable en droit international pénal.

### **Conclusion Poétique : Le *Mono no Aware* de notre Espèce**

Nous concluons ce rapport par un appel à la "conscience publique", sous la forme d'une réflexion finale ancrée dans le *mono no aware* — la tristesse empathique envers les choses éphémères. La vie humaine est ce qu'il y a de plus éphémère. Elle est unique, irréversible. Si nous laissons les machines décider qui doit mourir, nous aurons abdiqué notre propre droit à vivre en tant qu'êtres moraux. Nous serons devenus les outils de nos outils.

Silence des ruines \-  
La machine veille encore,  
Mais il n'y a plus d'hommes.

L'ONU a été créée pour "préserver les générations futures du fléau de la guerre". Aujourd'hui, elle doit les préserver du fléau d'une guerre sans humains. Il est immoral, illégal et suicidaire de confier l'épée de la justice à une main qui ne peut pas trembler. Agissez avant que l'hiver ne soit éternel.

**Fin du Rapport.**

#### **Sources des citations**

1. The Art of Concision: A Look at Haiku | Nippon.com, consulté le janvier 1, 2026, [https://www.nippon.com/en/nipponblog/m00086/](https://www.nippon.com/en/nipponblog/m00086/)  
2. Untranslatable Words: Mono No Aware, and the Aesthetics of Impermanence \- UTC, consulté le janvier 1, 2026, [https://www.utc.edu/document/80246](https://www.utc.edu/document/80246)  
3. International Discussions Concerning Lethal Autonomous Weapon Systems \- Congress.gov, consulté le janvier 1, 2026, [https://www.congress.gov/crs-product/IF11294](https://www.congress.gov/crs-product/IF11294)  
4. Convention on CCW GGE: Consideration of the Human Element in the Use of Lethal Force, consulté le janvier 1, 2026, [https://geneva.usmission.gov/2019/03/27/convention-on-certain-conventional-weapons-consideration-of-the-human-element-in-the-use-of-lethal-force/](https://geneva.usmission.gov/2019/03/27/convention-on-certain-conventional-weapons-consideration-of-the-human-element-in-the-use-of-lethal-force/)  
5. Biased Technology: The Automated Discrimination of Facial Recognition \- ACLU-MN.org, consulté le janvier 1, 2026, [https://www.aclu-mn.org/news/biased-technology-automated-discrimination-facial-recognition/](https://www.aclu-mn.org/news/biased-technology-automated-discrimination-facial-recognition/)  
6. Understanding bias in facial recognition technologies \- The Alan Turing Institute, consulté le janvier 1, 2026, [https://www.turing.ac.uk/sites/default/files/2020-10/understanding\_bias\_in\_facial\_recognition\_technology.pdf](https://www.turing.ac.uk/sites/default/files/2020-10/understanding_bias_in_facial_recognition_technology.pdf)  
7. The problem of algorithmic bias in AI-based military decision support systems, consulté le janvier 1, 2026, [https://blogs.icrc.org/law-and-policy/2024/09/03/the-problem-of-algorithmic-bias-in-ai-based-military-decision-support-systems/](https://blogs.icrc.org/law-and-policy/2024/09/03/the-problem-of-algorithmic-bias-in-ai-based-military-decision-support-systems/)  
8. A Hybrid Approach for Target Discrimination in Remote Sensing: Combining YOLO and CNN-Based Classifiers \- SciELO, consulté le janvier 1, 2026, [https://www.scielo.br/j/jatm/a/3wYpNRbVvqFCTD9k64jXMhL/](https://www.scielo.br/j/jatm/a/3wYpNRbVvqFCTD9k64jXMhL/)  
9. Research on Optical Remote Sensing Image Target Detection Technique Based on DCH-YOLOv7 Algorithm \- IEEE Xplore, consulté le janvier 1, 2026, [https://ieeexplore.ieee.org/iel7/6287639/10380310/10443451.pdf](https://ieeexplore.ieee.org/iel7/6287639/10380310/10443451.pdf)  
10. CHARLES UNIVERSITY Master's Thesis 2025 Daniela Kůstková, consulté le janvier 1, 2026, [https://dspace.cuni.cz/bitstream/handle/20.500.11956/196923/120498766.pdf?sequence=1\&isAllowed=y](https://dspace.cuni.cz/bitstream/handle/20.500.11956/196923/120498766.pdf?sequence=1&isAllowed=y)  
11. Remote Sensing of Target Object Detection and Identification II \- MDPI, consulté le janvier 1, 2026, [https://www.mdpi.com/2072-4292/16/16/3106](https://www.mdpi.com/2072-4292/16/16/3106)  
12. A Method of SAR Image Automatic Target Recognition Based on Convolution Auto-Encode and Support Vector Machine \- MDPI, consulté le janvier 1, 2026, [https://www.mdpi.com/2072-4292/14/21/5559](https://www.mdpi.com/2072-4292/14/21/5559)  
13. Full article: The legal and ethical implications of drone warfare \- Taylor & Francis, consulté le janvier 1, 2026, [https://www.tandfonline.com/doi/full/10.1080/13642987.2014.991210](https://www.tandfonline.com/doi/full/10.1080/13642987.2014.991210)  
14. Reforming U.S. Drone Strike Policies \- Council on Foreign Relations, consulté le janvier 1, 2026, [https://cdn.cfr.org/sites/default/files/pdf/2012/12/Drones\_CSR65.pdf](https://cdn.cfr.org/sites/default/files/pdf/2012/12/Drones_CSR65.pdf)  
15. Report: U.S. drone strike may have killed up to a dozen civilians in Yemen | PBS News, consulté le janvier 1, 2026, [https://www.pbs.org/newshour/world/report-u-s-drones-may-killed-civilians](https://www.pbs.org/newshour/world/report-u-s-drones-may-killed-civilians)  
16. A Wedding That Became a Funeral: US Drone Attack on Marriage Procession in Yemen, consulté le janvier 1, 2026, [https://www.hrw.org/report/2014/02/19/wedding-became-funeral/us-drone-attack-marriage-procession-yemen](https://www.hrw.org/report/2014/02/19/wedding-became-funeral/us-drone-attack-marriage-procession-yemen)  
17. AI and the Actual IHL Accountability Gap \- Centre for International Governance Innovation, consulté le janvier 1, 2026, [https://www.cigionline.org/articles/ai-and-the-actual-ihl-accountability-gap/](https://www.cigionline.org/articles/ai-and-the-actual-ihl-accountability-gap/)  
18. 'Lavender': The AI machine directing Israel's bombing spree in Gaza \- \+972 Magazine, consulté le janvier 1, 2026, [https://www.972mag.com/lavender-ai-israeli-army-gaza/](https://www.972mag.com/lavender-ai-israeli-army-gaza/)  
19. AI-assisted targeting in the Gaza Strip \- Wikipedia, consulté le janvier 1, 2026, [https://en.wikipedia.org/wiki/AI-assisted\_targeting\_in\_the\_Gaza\_Strip](https://en.wikipedia.org/wiki/AI-assisted_targeting_in_the_Gaza_Strip)  
20. 'A mass assassination factory': Inside Israel's calculated bombing of Gaza \- \+972 Magazine, consulté le janvier 1, 2026, [https://www.972mag.com/mass-assassination-factory-israel-calculated-bombing-gaza/](https://www.972mag.com/mass-assassination-factory-israel-calculated-bombing-gaza/)  
21. The Lavender precedent: automated kill lists and the limits of International Humanitarian Law \- AOAV, consulté le janvier 1, 2026, [https://aoav.org.uk/2025/the-lavender-precedent-automated-kill-lists-and-the-limits-of-international-humanitarian-law/](https://aoav.org.uk/2025/the-lavender-precedent-automated-kill-lists-and-the-limits-of-international-humanitarian-law/)  
22. Gaza, Artificial Intelligence, and Kill Lists, consulté le janvier 1, 2026, [https://verfassungsblog.de/gaza-artificial-intelligence-and-kill-lists/](https://verfassungsblog.de/gaza-artificial-intelligence-and-kill-lists/)  
23. Israel-Hamas 2024 Symposium \- AI-Based Targeting in Gaza: Surveying Expert Responses and Refining the Debate \- Lieber Institute, consulté le janvier 1, 2026, [https://lieber.westpoint.edu/ai-based-targeting-gaza-surveying-expert-responses-refining-debate/](https://lieber.westpoint.edu/ai-based-targeting-gaza-surveying-expert-responses-refining-debate/)  
24. Living Under Drones | Stanford Law School, consulté le janvier 1, 2026, [https://law.stanford.edu/wp-content/uploads/2015/07/Stanford-NYU-LIVING-UNDER-DRONES.pdf](https://law.stanford.edu/wp-content/uploads/2015/07/Stanford-NYU-LIVING-UNDER-DRONES.pdf)  
25. Algorithms of war: The use of artificial intelligence in decision making in armed conflict, consulté le janvier 1, 2026, [https://blogs.icrc.org/law-and-policy/2023/10/24/algorithms-of-war-use-of-artificial-intelligence-decision-making-armed-conflict/](https://blogs.icrc.org/law-and-policy/2023/10/24/algorithms-of-war-use-of-artificial-intelligence-decision-making-armed-conflict/)  
26. Stop Killer Robots – Less Autonomy, More humanity., consulté le janvier 1, 2026, [https://www.stopkillerrobots.org/](https://www.stopkillerrobots.org/)  
27. Problems with autonomous weapons \- Stop Killer Robots, consulté le janvier 1, 2026, [https://www.stopkillerrobots.org/stop-killer-robots/facts-about-autonomous-weapons/](https://www.stopkillerrobots.org/stop-killer-robots/facts-about-autonomous-weapons/)  
28. Chapter: 5 Combatant Identification in Urban Warfare \- National Academies of Sciences, Engineering, and Medicine, consulté le janvier 1, 2026, [https://www.nationalacademies.org/read/11286/chapter/7](https://www.nationalacademies.org/read/11286/chapter/7)  
29. Russia leads an assault on progress at UN discussions, the CCW has failed., consulté le janvier 1, 2026, [https://www.stopkillerrobots.org/news/russia-leads-an-assault-on-progress-at-un-discussions-the-ccw-has-failed/](https://www.stopkillerrobots.org/news/russia-leads-an-assault-on-progress-at-un-discussions-the-ccw-has-failed/)  
30. Banning Autonomous Weapons: A Legal and Ethical Mandate \- NDLScholarship, consulté le janvier 1, 2026, [https://scholarship.law.nd.edu/cgi/viewcontent.cgi?article=2858\&context=law\_faculty\_scholarship](https://scholarship.law.nd.edu/cgi/viewcontent.cgi?article=2858&context=law_faculty_scholarship)  
31. The Future of Warfare: National Positions on the Governance of Lethal Autonomous Weapons Systems \- Lieber Institute, consulté le janvier 1, 2026, [https://lieber.westpoint.edu/future-warfare-national-positions-governance-lethal-autonomous-weapons-systems/](https://lieber.westpoint.edu/future-warfare-national-positions-governance-lethal-autonomous-weapons-systems/)  
32. U.S., Russia Impede Steps to Ban 'Killer Robots' | Arms Control Association, consulté le janvier 1, 2026, [https://www.armscontrol.org/act/2018-10/news/us-russia-impede-steps-ban-killer-robots](https://www.armscontrol.org/act/2018-10/news/us-russia-impede-steps-ban-killer-robots)  
33. Stopping Killer Robots: Country Positions on Banning Fully Autonomous Weapons and Retaining Human Control | HRW, consulté le janvier 1, 2026, [https://www.hrw.org/report/2020/08/10/stopping-killer-robots/country-positions-banning-fully-autonomous-weapons-and](https://www.hrw.org/report/2020/08/10/stopping-killer-robots/country-positions-banning-fully-autonomous-weapons-and)  
34. CCW Report \- Reaching Critical Will, consulté le janvier 1, 2026, [https://reachingcriticalwill.org/disarmament-fora/ccw/2025/laws/ccwreport/17475](https://reachingcriticalwill.org/disarmament-fora/ccw/2025/laws/ccwreport/17475)  
35. Statement by ICRC President Mirjana Spoljaric to the 'Vienna Conference on Autonomous Weapon Systems 2024: Humanity at the Crossroads' | International Committee of the Red Cross, consulté le janvier 1, 2026, [https://www.icrc.org/en/document/statement-icrc-president-mirjana-spoljaric-vienna-conference-autonomous-weapon-systems-2024](https://www.icrc.org/en/document/statement-icrc-president-mirjana-spoljaric-vienna-conference-autonomous-weapon-systems-2024)  
36. Landmark joint call from UN Secretary-General and ICRC President \- Stop Killer Robots, consulté le janvier 1, 2026, [https://www.stopkillerrobots.org/news/landmark-joint-call/](https://www.stopkillerrobots.org/news/landmark-joint-call/)  
37. REMARKS: Banning 'Killer Robots': The Legal Obligations of the Martens Clause, consulté le janvier 1, 2026, [https://www.armscontrol.org/act/2018-10/features/remarks-banning-killer-robots-legal-obligations-martens-clause](https://www.armscontrol.org/act/2018-10/features/remarks-banning-killer-robots-legal-obligations-martens-clause)  
38. Heed the Call: A Moral and Legal Imperative to Ban Killer Robots | HRW, consulté le janvier 1, 2026, [https://www.hrw.org/report/2018/08/21/heed-call/moral-and-legal-imperative-ban-killer-robots](https://www.hrw.org/report/2018/08/21/heed-call/moral-and-legal-imperative-ban-killer-robots)  
39. A Hazard to Human Rights: Autonomous Weapons Systems and Digital Decision-Making, consulté le janvier 1, 2026, [https://www.hrw.org/report/2025/04/28/a-hazard-to-human-rights/autonomous-weapons-systems-and-digital-decision-making](https://www.hrw.org/report/2025/04/28/a-hazard-to-human-rights/autonomous-weapons-systems-and-digital-decision-making)  
40. Lethal Autonomous Weapon System 'Politically Unacceptable, Morally Repugnant and Should Be Banned', Secretary-General Says during Informal Consultations on Issue, consulté le janvier 1, 2026, [https://press.un.org/en/2025/sgsm22643.doc.htm](https://press.un.org/en/2025/sgsm22643.doc.htm)  
41. UN Secretary-General reiterates call for action on autonomous weapons, consulté le janvier 1, 2026, [https://automatedresearch.org/news/un-secretary-general-reiterates-call-for-action-on-autonomous-weapons/](https://automatedresearch.org/news/un-secretary-general-reiterates-call-for-action-on-autonomous-weapons/)  
42. AI must serve human potential, not compete against it, pope says | USCCB, consulté le janvier 1, 2026, [https://www.usccb.org/news/2023/ai-must-serve-human-potential-not-compete-against-it-pope-says](https://www.usccb.org/news/2023/ai-must-serve-human-potential-not-compete-against-it-pope-says)  
43. LVII World Day of Peace 2024 \- Artificial Intelligence and Peace \- The Holy See, consulté le janvier 1, 2026, [https://www.vatican.va/content/francesco/en/messages/peace/documents/20231208-messaggio-57giornatamondiale-pace2024.html](https://www.vatican.va/content/francesco/en/messages/peace/documents/20231208-messaggio-57giornatamondiale-pace2024.html)  
44. Tech workers and killer robots, consulté le janvier 1, 2026, [https://www.stopkillerrobots.org/tech-workers-and-killer-robots/](https://www.stopkillerrobots.org/tech-workers-and-killer-robots/)